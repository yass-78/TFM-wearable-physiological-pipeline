{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a74c409-3f54-492c-89d4-0dfa5949f7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# AVRO ingestion -> all_data.pkl\n",
    "#\n",
    "# - One Spark session per patient\n",
    "# - AVRO files are sorted chronologically\n",
    "# - All rows from rawData are extracted per AVRO\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import hashlib\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Base path containing all patient folders\n",
    "# ------------------------------------------------------------\n",
    "base_path = \"/srv/timebase/EmbracePlus2\"\n",
    "\n",
    "patient_dirs = [\n",
    "    os.path.join(base_path, d)\n",
    "    for d in os.listdir(base_path)\n",
    "    if os.path.isdir(os.path.join(base_path, d))\n",
    "]\n",
    "\n",
    "# Final container: patient_id -> list of rawData blocks\n",
    "all_data = {}\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Iterate over all patients\n",
    "# ------------------------------------------------------------\n",
    "for patient_path in patient_dirs:\n",
    "    patient_id = os.path.basename(patient_path)\n",
    "    print(f\"[INFO] Processing patient {patient_id}...\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Locate AVRO files under raw_data/v6\n",
    "    # --------------------------------------------------------\n",
    "    avro_files = []\n",
    "    for root, _, files in os.walk(patient_path):\n",
    "        for f in files:\n",
    "            if f.endswith(\".avro\") and \"raw_data/v6\" in root:\n",
    "                avro_files.append(os.path.join(root, f))\n",
    "\n",
    "    if not avro_files:\n",
    "        print(f\"[WARN] No AVRO files found for {patient_id}\")\n",
    "        continue\n",
    "\n",
    "    # Ensure deterministic processing order\n",
    "    avro_files.sort()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Create one Spark session per patient\n",
    "    # --------------------------------------------------------\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(f\"AVRO_{patient_id}\")\n",
    "        .master(\"local\")\n",
    "        .config(\"spark.driver.memory\", \"8G\")\n",
    "        .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.5.0\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    blocks = []\n",
    "    n_total_rows = 0\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Read each AVRO file sequentially\n",
    "    # --------------------------------------------------------\n",
    "    for i, file in enumerate(avro_files, start=1):\n",
    "        print(f\"[INFO] Reading file {i}/{len(avro_files)}: {os.path.basename(file)}\")\n",
    "\n",
    "        try:\n",
    "            df = spark.read.format(\"avro\").load(file)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to read {os.path.basename(file)}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Check expected column\n",
    "        if \"rawData\" not in df.columns:\n",
    "            print(f\"[WARN] Column 'rawData' not found in {os.path.basename(file)}\")\n",
    "            continue\n",
    "\n",
    "        # Collect all rawData rows\n",
    "        rows = df.select(col(\"rawData\")).collect()\n",
    "\n",
    "        if not rows:\n",
    "            print(f\"[WARN] Empty AVRO file: {os.path.basename(file)}\")\n",
    "            continue\n",
    "\n",
    "        for row in rows:\n",
    "            rd = row[\"rawData\"]\n",
    "            if rd is not None:\n",
    "                blocks.append(rd)\n",
    "                n_total_rows += 1\n",
    "\n",
    "        # Explicit cleanup to reduce memory footprint\n",
    "        del df, rows\n",
    "        gc.collect()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Stop Spark session for this patient\n",
    "    # --------------------------------------------------------\n",
    "    spark.stop()\n",
    "    print(f\"[OK] Spark session closed for {patient_id}\")\n",
    "\n",
    "    # Store patient data\n",
    "    all_data[patient_id] = blocks\n",
    "    print(f\"[OK] Patient {patient_id} added with {len(blocks)} blocks (rows collected: {n_total_rows})\")\n",
    "\n",
    "    del blocks, n_total_rows\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Persist full dataset to disk\n",
    "# ------------------------------------------------------------\n",
    "output_path = os.path.expanduser(\"~/TFM/all_data.pkl\")\n",
    "with open(output_path, \"wb\") as f:\n",
    "    pickle.dump(all_data, f)\n",
    "\n",
    "print(f\"[OK] all_data.pkl saved with {len(all_data)} patients\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
